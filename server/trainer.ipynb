{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from os import listdir\n",
    "\n",
    "from src.db_registry import registry\n",
    "from src.pre_calculate_reward import PreCalculateReward\n",
    "from src.config import *\n",
    "from src.q_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed reward pre calculation for 0 races\n"
     ]
    }
   ],
   "source": [
    "pre_calculate_reward = PreCalculateReward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLAY_BUFFER_MERGED_ELEMENT_SPEC =  \\\n",
    "            (tf.TensorSpec((135, 180, 3), dtype=tf.uint8),\n",
    "             (tf.TensorSpec((3,), dtype=tf.uint8),\n",
    "              tf.TensorSpec((3,), dtype=tf.uint8)))\n",
    "\n",
    "\n",
    "REWARD_BUFFER_ELEMENT_SPEC =  \\\n",
    "            (tf.TensorSpec((), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningStep:\n",
    "    def __init__(self, lr, epochs, use_discount):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.use_discount = use_discount\n",
    "        \n",
    "    def get_lr(self):\n",
    "        return self.lr\n",
    "    \n",
    "    def get_epochs(self):\n",
    "        return self.epochs\n",
    "    \n",
    "    def get_use_discount(self):\n",
    "        return self.use_discount\n",
    "    \n",
    "    \n",
    "learning_step_collection = [[LearningStep(0.00025, 3, 0),\n",
    "                            LearningStep(0.000025, 1, 0),\n",
    "                            LearningStep(0.00025, 3, 1),\n",
    "                            LearningStep(0.000025, 1, 1)]]\n",
    "                            \n",
    "\n",
    "start_skip = 5\n",
    "skips = 1\n",
    "    \n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop\n",
    "loss = \"mse\" #tf.keras.losses.Huber()\n",
    "\n",
    "q_model_description = \"deep convolutional network v3 disc 0.99 ABSOLUTE SARSA 5/ 1 skips, 0.00025 x 3, 0.000025 x 1(auto norm)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(filename):\n",
    "    dataset_base = tf.data.experimental.load(REPLAY_BUFFER_PATH + filename, REPLAY_BUFFER_MERGED_ELEMENT_SPEC,\n",
    "                                            compression=\"GZIP\")\n",
    "    dataset_reward = tf.data.experimental.load(REWARD_BUFFER_PATH + filename, REWARD_BUFFER_ELEMENT_SPEC)\n",
    "    \n",
    "    dataset_base = tf.data.Dataset.zip(\n",
    "        (dataset_base, dataset_base.skip(start_skip + skips)))\n",
    "    dataset_reward =  tf.data.Dataset.zip(\n",
    "        tuple([dataset_reward] + [dataset_reward.skip(i + start_skip + 1) for i in range(skips)]))\n",
    "    \n",
    "    dataset = tf.data.Dataset.zip((dataset_base, dataset_reward))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def batch_calculations_dataset(*x):\n",
    "    with tf.device(\"gpu:0\"):\n",
    "        zipped_rewards = [i for i in x[1]][1:]\n",
    "        \n",
    "        rewards = tf.stack(zipped_rewards, axis=0)\n",
    "        \n",
    "        discounts = tf.constant([DISCOUNT_FACTOR**(i + start_skip) for i in range(len((zipped_rewards)))])\n",
    "        \n",
    "        discounted_rewards = tf.transpose(rewards) * discounts\n",
    "        \n",
    "        rewards = tf.math.reduce_sum(tf.transpose(discounted_rewards), axis=0)\n",
    "    \n",
    "        rewards = (rewards - norm_mean) / norm_stdev\n",
    "        \n",
    "        rewards = tf.reshape(rewards, [-1, 1])\n",
    "        \n",
    "        if use_discount == 1:\n",
    "            q_discount = DISCOUNT_FACTOR ** skips\n",
    "            \n",
    "            next_reward_params = target_q_model(tf.cast(x[0][1][0], dtype=tf.float32) / 255.0)\n",
    "\n",
    "            next_y0 = tf.math.reduce_max(next_reward_params[:][0], axis=1)\n",
    "            next_y1 = tf.math.reduce_max(next_reward_params[:][1], axis=1)\n",
    "            next_y0 = tf.reshape(next_y0 * q_discount, [-1, 1])\n",
    "            next_y1 = tf.reshape(next_y1 * q_discount, [-1, 1])\n",
    "\n",
    "            rewards = (rewards + next_y0)\n",
    "            rewards = (rewards - norm_mean_discounted) / norm_stdev_discounted\n",
    "        \n",
    "        actions = (tf.cast(x[0][0][1][:][0], dtype=tf.float32),\n",
    "                  tf.cast(x[0][0][1][:][1], dtype=tf.float32))\n",
    "        \n",
    "        return tf.cast(x[0][0][0], dtype=tf.float32) / 255.0, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    filenames = [obj for obj in listdir(REWARD_BUFFER_PATH)]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "    dataset = dataset.shuffle(len(filenames))\n",
    "    dataset = dataset.flat_map(preprocess_dataset)\n",
    "    dataset = dataset.shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "    dataset = dataset.batch(BATCH_SIZE_Q_MODEL_TRAINING).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.map(batch_calculations_dataset, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_discount = tf.Variable(0, dtype=tf.int32)\n",
    "norm_mean = tf.Variable(0.0, dtype=tf.float32)\n",
    "norm_stdev = tf.Variable(1.0, dtype=tf.float32)\n",
    "\n",
    "norm_mean_discounted = tf.Variable(0.0, dtype=tf.float32)\n",
    "norm_stdev_discounted = tf.Variable(1.0, dtype=tf.float32)\n",
    "\n",
    "target_q_model = deep_convolutional_network_v3()\n",
    "norm_layer = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "\n",
    "\n",
    "def dataset_reward(*x):\n",
    "    return x[2]\n",
    "\n",
    "\n",
    "def adapt_normalization():\n",
    "    norm_layer.adapt(get_dataset().map(dataset_reward).take(NORMALIZATION_BATCH_COUNT))\n",
    "    mean = norm_layer.mean.numpy()[0]\n",
    "    stdev = norm_layer.variance.numpy()[0]**0.5\n",
    "    norm_mean.assign(mean)\n",
    "    norm_stdev.assign(stdev)\n",
    "    print(\"Reward normalization:\")\n",
    "    print(\"mean: \", mean)\n",
    "    print(\"stdev:\", stdev)\n",
    "    \n",
    "    \n",
    "def adapt_discounted_normalization():\n",
    "    norm_layer.adapt(get_dataset().map(dataset_reward).take(NORMALIZATION_BATCH_COUNT))\n",
    "    mean = norm_layer.mean.numpy()[0]\n",
    "    stdev = norm_layer.variance.numpy()[0]**0.5\n",
    "    norm_mean_discounted.assign(mean)\n",
    "    norm_stdev_discounted.assign(stdev)\n",
    "    print(\"Discounted reward normalization:\")\n",
    "    print(\"mean: \", mean)\n",
    "    print(\"stdev:\", stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward normalization:\n",
      "mean:  16.299524\n",
      "stdev: 11.170251602986543\n",
      "\n",
      "49/49 [==============================] - 4s 74ms/step - loss: 0.6055 - output_1_loss: 0.2825 - output_2_loss: 0.3230\n",
      "WARNING:tensorflow:From c:\\users\\stijn\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From c:\\users\\stijn\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: D:/ml_mk/tony_kart/q_models/DESKTOP-IISUQP6_20210305_1813441\\assets\n",
      "\n",
      "49/49 [==============================] - 3s 63ms/step - loss: 0.1830 - output_1_loss: 0.0969 - output_2_loss: 0.08610.1919 - output_1_loss: 0.0982 - out\n",
      "INFO:tensorflow:Assets written to: D:/ml_mk/tony_kart/q_models/DESKTOP-IISUQP6_20210305_1813442\\assets\n",
      "\n",
      "49/49 [==============================] - 3s 63ms/step - loss: 0.1122 - output_1_loss: 0.0594 - output_2_loss: 0.0528\n",
      "INFO:tensorflow:Assets written to: D:/ml_mk/tony_kart/q_models/DESKTOP-IISUQP6_20210305_1813443\\assets\n",
      "\n",
      "49/49 [==============================] - 3s 63ms/step - loss: 0.0595 - output_1_loss: 0.0305 - output_2_loss: 0.0290\n",
      "INFO:tensorflow:Assets written to: D:/ml_mk/tony_kart/q_models/DESKTOP-IISUQP6_20210305_1813444\\assets\n",
      "Discounted reward normalization:\n",
      "mean:  0.23085572\n",
      "stdev: 1.6337999342370273\n",
      "DESKTOP-IISUQP6_20210305_1813444\n",
      "49/49 [==============================] - 4s 82ms/step - loss: 0.5104 - output_1_loss: 0.2458 - output_2_loss: 0.2646\n",
      "INFO:tensorflow:Assets written to: D:/ml_mk/tony_kart/q_models/DESKTOP-IISUQP6_20210305_1813445\\assets\n",
      "DESKTOP-IISUQP6_20210305_1813444\n",
      "49/49 [==============================] - 4s 81ms/step - loss: 0.1348 - output_1_loss: 0.0634 - output_2_loss: 0.0715\n",
      "INFO:tensorflow:Assets written to: D:/ml_mk/tony_kart/q_models/DESKTOP-IISUQP6_20210305_1813446\\assets\n",
      "DESKTOP-IISUQP6_20210305_1813444\n",
      "      2/Unknown - 0s 90ms/step - loss: 0.1001 - output_1_loss: 0.0424 - output_2_loss: 0.0577WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0284s vs `on_train_batch_end` time: 0.0555s). Check your callbacks.\n",
      "49/49 [==============================] - 4s 81ms/step - loss: 0.0882 - output_1_loss: 0.0406 - output_2_loss: 0.0477\n",
      "INFO:tensorflow:Assets written to: D:/ml_mk/tony_kart/q_models/DESKTOP-IISUQP6_20210305_1813447\\assets\n",
      "DESKTOP-IISUQP6_20210305_1813444\n",
      "      2/Unknown - 0s 59ms/step - loss: 0.0680 - output_1_loss: 0.0317 - output_2_loss: 0.0363WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0329s vs `on_train_batch_end` time: 0.0523s). Check your callbacks.\n",
      "49/49 [==============================] - 4s 80ms/step - loss: 0.0460 - output_1_loss: 0.0212 - output_2_loss: 0.02480.0463 - output_1_loss: 0.0214 - output_2_loss: 0.\n",
      "INFO:tensorflow:Assets written to: D:/ml_mk/tony_kart/q_models/DESKTOP-IISUQP6_20210305_1813448\\assets\n",
      " \n",
      "model saved:  DESKTOP-IISUQP6_20210305_1813448\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for learning_steps in learning_step_collection:\n",
    "    adapt_normalization()\n",
    "    \n",
    "    q_model = deep_convolutional_network_v3()\n",
    "    \n",
    "    q_model.compile(optimizer=\"RMSprop\", loss=[loss, loss])\n",
    "    #q_model.fit(get_dataset().take(1))\n",
    "    #q_model.summary()\n",
    "    \n",
    "    target_q_model = q_model\n",
    "    \n",
    "    models_created = 0\n",
    "    model_id = get_new_id()\n",
    "    target_q_model_id = \"\"\n",
    "    use_discount = tf.Variable(0, dtype=tf.int32)\n",
    "    lr = 0.0000001\n",
    "\n",
    "    for learning_step in learning_steps:\n",
    "        if learning_step.get_lr() > lr and learning_step.get_use_discount():\n",
    "            registry.add_q_model(model_id + str(models_created), \n",
    "                                 datetime.now().timestamp(), q_model_description)\n",
    "            target_q_model_id = model_id + str(models_created) \n",
    "            \n",
    "            target_q_model = tf.keras.models.load_model(Q_MODEL_PATH + target_q_model_id)\n",
    "            q_model = deep_convolutional_network_v3()\n",
    "            q_model.compile(optimizer=\"RMSprop\", loss=[loss, loss])\n",
    "            \n",
    "            use_discount.assign(learning_step.get_use_discount())\n",
    "            adapt_discounted_normalization()\n",
    "        \n",
    "        lr = learning_step.get_lr()\n",
    "        \n",
    "        q_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_step.get_lr()), loss=[loss, loss])\n",
    "\n",
    "        for epoch in range(learning_step.get_epochs()):\n",
    "            print(target_q_model_id)\n",
    "            q_model.fit(get_dataset())\n",
    "\n",
    "            models_created += 1\n",
    "            q_model.save(Q_MODEL_PATH + model_id + str(models_created))\n",
    "            \n",
    "    registry.add_q_model(model_id + str(models_created), datetime.now().timestamp(), q_model_description)\n",
    "    print(\" \")\n",
    "    print(\"model saved: \", model_id + str(models_created))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "min0:  -0.57516426\n",
      "max0:  2.3428164\n",
      "==============\n",
      "min0:  -0.8193543\n",
      "max0:  2.7135696\n",
      "==============\n",
      "min0:  -1.1331646\n",
      "max0:  2.7135696\n",
      "==============\n",
      "min0:  -1.4778433\n",
      "max0:  2.7135696\n"
     ]
    }
   ],
   "source": [
    "use_discount.assign(1)\n",
    "\n",
    "loop_count = 0\n",
    "\n",
    "min0 = 1.0\n",
    "max0 = 0.0\n",
    "\n",
    "for elem in dataset_test:\n",
    "    loop_count += 1\n",
    "    y0 = tf.math.reduce_max(elem[1][0], axis=1) * elem[2][0]\n",
    "    \n",
    "    min0 = min(min0, min(y0).numpy())\n",
    "    max0 = max(max0, max(y0).numpy())\n",
    "    \n",
    "\n",
    "    if loop_count % 10 == 0:\n",
    "        print(\"==============\")\n",
    "        print(\"min0: \", min0)\n",
    "        print(\"max0: \", max0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
